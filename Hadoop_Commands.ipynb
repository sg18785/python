{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0G40cspw9vFafRZEHb9rP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sg18785/python/blob/main/Hadoop_Commands.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement 1"
      ],
      "metadata": {
        "id": "rgHt41Hqei_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a folder in HDFS by name “dir01” and move input1.txt , input2.txt and input3.txt into\n",
        "/dir01."
      ],
      "metadata": {
        "id": "et1w5Na5e4VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hdfs dfs -mkdir /dir01\n",
        "\n",
        "hdfs dfs -put input1.txt input2.txt input3.txt /dir01"
      ],
      "metadata": {
        "id": "3J4kx472fAza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. List only the file names present in “/dir01”"
      ],
      "metadata": {
        "id": "HDGCt7hOfegN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hdfs dfs -ls /dir01 | awk '{print $NF}'"
      ],
      "metadata": {
        "id": "aVjVH7e1fRCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Change the replication factor for the content present in directory “/dir01” to 5 and display\n",
        "the replication factor for the files present in “/dir01”."
      ],
      "metadata": {
        "id": "C9Uw3x7fgfrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hdfs dfs -setrep -w 5 /dir01"
      ],
      "metadata": {
        "id": "gU36-r9LglGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create a folder in HDFS by name “scenario01” and create directory “level01” inside\n",
        "“scenario01” directory and create another directory “level02” inside directory “level01”.\n",
        "Once the required directories are created copy input1.txt to scenario01, input2.txt to level01\n",
        "and input3.txt to level02 and finally recursively print only the file names present in\n",
        "directory scenario01\n"
      ],
      "metadata": {
        "id": "0L-VLC8hg2OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the \"scenario01\" directory and \"level01\" inside it\n",
        "hdfs dfs -mkdir /scenario01\n",
        "hdfs dfs -mkdir /scenario01/level01\n",
        "\n",
        "# Create \"level02\" inside \"level01\"\n",
        "hdfs dfs -mkdir /scenario01/level01/level02\n",
        "\n",
        "# Copy the files to their respective directories\n",
        "hdfs dfs -put input1.txt /scenario01\n",
        "hdfs dfs -put input2.txt /scenario01/level01\n",
        "hdfs dfs -put input3.txt /scenario01/level01/level02\n",
        "\n",
        "\n",
        "# Recursively list the file names in \"scenario01\"\n",
        "hdfs dfs -ls -R /scenario01 | grep -v \"^d\" | awk '{print $NF}'"
      ],
      "metadata": {
        "id": "NeHWqZkBhFpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement 02"
      ],
      "metadata": {
        "id": "5nZH6Skvkfot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Run MapReduce Program and capture the application Id of the job.\n"
      ],
      "metadata": {
        "id": "AozIxNKyjacp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hadoop jar mapreduce_job.jar com.example.MyMapReduceJob /input /output 2>&1 | grep \"application_\" | awk '{print $NF}'\n"
      ],
      "metadata": {
        "id": "d7lphCbAkOnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Re-run the MapReduce program and kill the application using the yarn command."
      ],
      "metadata": {
        "id": "fk2tF_NIkj3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hadoop jar my-mapreduce-job.jar com.example.MyMapReduceJob /input /output\n",
        "\n",
        "yarn application -list\n",
        "\n",
        "yarn application -kill <application-id>\n"
      ],
      "metadata": {
        "id": "wjHWMGzpkivM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. List all the applications which are RUNNING state"
      ],
      "metadata": {
        "id": "RM5Mm5XNloWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yarn application -list -appStates RUNNING\n"
      ],
      "metadata": {
        "id": "I3vS4esvmkWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. View the logs of any of the jobs which are already completed"
      ],
      "metadata": {
        "id": "jix9xg80mlrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yarn application -list -appStates FINISHED\n"
      ],
      "metadata": {
        "id": "xiS6k1kNlfWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eizOj0dAmpwq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}